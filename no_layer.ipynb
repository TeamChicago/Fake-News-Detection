{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "real_data = pd.read_csv('True.csv')\n",
    "fake_data = pd.read_csv('Fake.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  \n",
       "0  December 31, 2017  \n",
       "1  December 31, 2017  \n",
       "2  December 30, 2017  \n",
       "3  December 29, 2017  \n",
       "4  December 25, 2017  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake데이터와 true데이터의 개수 동일하게 설정\n",
    "nb_articles = min(len(real_data), len(fake_data))\n",
    "real_data = real_data[:nb_articles]\n",
    "fake_data = fake_data[:nb_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row에 'is_fake' row 추가하여 값 초기화\n",
    "real_data['is_fake'] = False\n",
    "fake_data['is_fake'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>is_fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "\n",
       "                 date  is_fake  \n",
       "0  December 31, 2017     False  \n",
       "1  December 29, 2017     False  \n",
       "2  December 31, 2017     False  \n",
       "3  December 30, 2017     False  \n",
       "4  December 29, 2017     False  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>is_fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  is_fake  \n",
       "0  December 31, 2017     True  \n",
       "1  December 31, 2017     True  \n",
       "2  December 30, 2017     True  \n",
       "3  December 29, 2017     True  \n",
       "4  December 25, 2017     True  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>is_fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Five killed in sectarian attack in Pakistan</td>\n",
       "      <td>QUETTA, Pakistan (Reuters) - (This October 9 s...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>October 9, 2017</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>May's party suspends two EU lawmakers over Bre...</td>\n",
       "      <td>BRUSSELS (Reuters) - Britain s ruling Conserva...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>October 8, 2017</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Factbox: Trump on Twitter (Sept 20) - Graham-C...</td>\n",
       "      <td>The following statements were posted to the ve...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>September 20, 2017</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hostility grows towards Syrian refugees in Leb...</td>\n",
       "      <td>BEIRUT (Reuters) - Abu Yazan has rarely steppe...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 28, 2017</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Atlantic City mayor terms N.J. takeover plan '...</td>\n",
       "      <td>ATLANTIC CITY, N.J. (Reuters) - Atlantic City’...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>February 22, 2016</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0        Five killed in sectarian attack in Pakistan   \n",
       "1  May's party suspends two EU lawmakers over Bre...   \n",
       "2  Factbox: Trump on Twitter (Sept 20) - Graham-C...   \n",
       "3  Hostility grows towards Syrian refugees in Leb...   \n",
       "4  Atlantic City mayor terms N.J. takeover plan '...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  QUETTA, Pakistan (Reuters) - (This October 9 s...     worldnews   \n",
       "1  BRUSSELS (Reuters) - Britain s ruling Conserva...     worldnews   \n",
       "2  The following statements were posted to the ve...  politicsNews   \n",
       "3  BEIRUT (Reuters) - Abu Yazan has rarely steppe...     worldnews   \n",
       "4  ATLANTIC CITY, N.J. (Reuters) - Atlantic City’...  politicsNews   \n",
       "\n",
       "                  date  is_fake  \n",
       "0     October 9, 2017     False  \n",
       "1     October 8, 2017     False  \n",
       "2  September 20, 2017     False  \n",
       "3     August 28, 2017     False  \n",
       "4   February 22, 2016     False  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "data = pd.concat([real_data, fake_data])\n",
    "\n",
    "# fake, true 데이터를 합치고, 무작위 셔플\n",
    "data = shuffle(data).reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set : 25700\n",
      "Size of validation set: 8567\n",
      "Size of testing set: 8567\n"
     ]
    }
   ],
   "source": [
    "# 학습데이터 60%, 검증데이터 20%, 테스트데이터 20% 로 분리\n",
    "train_data, validate_data, test_data = np.split(data.sample(frac=1), [int(.6*len(data)), int(.8*len(data))])\n",
    "\n",
    "# reset_index하여 각 데이터별 인덱스 정렬\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "validate_data = validate_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "del real_data\n",
    "del fake_data\n",
    "\n",
    "print(\"Size of training set : {}\".format(len(train_data)))\n",
    "print(\"Size of validation set: {}\".format(len(validate_data)))\n",
    "print(\"Size of testing set: {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Requirement already satisfied: transformers in c:\\users\\user\\anaconda3\\lib\\site-packages (4.4.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.0.44)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (4.59.0)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (2021.3.17)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: six in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!conda install -y pytorch torchvision cudatoolkit=10.1 -c pytorch\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.cuda.is_available()\n",
    "# 쿠다없으면 cpu 사용\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "model.config.num_labels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the pre trained parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.SGD(model.classifier.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    parts = []\n",
    "\n",
    "    text_len = len(text.split(' '))\n",
    "    delta = 300\n",
    "    max_parts = 5\n",
    "    nb_cuts = int(text_len / delta)\n",
    "    nb_cuts = min(nb_cuts, max_parts)\n",
    "    \n",
    "    \n",
    "    for i in range(nb_cuts + 1):\n",
    "        text_part = ' '.join(text.split(' ')[i * delta: (i + 1) * delta])\n",
    "        parts.append(tokenizer.encode(text_part, return_tensors=\"pt\", max_length=500).to(device))\n",
    "\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/25700. Average loss: 0.2574516536295414\n",
      "600/25700. Average loss: 0.2625815826530258\n",
      "900/25700. Average loss: 0.2609694382486244\n",
      "1200/25700. Average loss: 0.27644607910265523\n",
      "1500/25700. Average loss: 0.26010702732329566\n",
      "1800/25700. Average loss: 0.2695156510019054\n",
      "2100/25700. Average loss: 0.2676220900642996\n",
      "2400/25700. Average loss: 0.26177932251865665\n",
      "2700/25700. Average loss: 0.2726197820405165\n",
      "3000/25700. Average loss: 0.2647109723463654\n",
      "3300/25700. Average loss: 0.2547325244049231\n",
      "3600/25700. Average loss: 0.2635251714165012\n",
      "3900/25700. Average loss: 0.2631588336452842\n",
      "4200/25700. Average loss: 0.27215076972730456\n",
      "4500/25700. Average loss: 0.2591610004007816\n",
      "4800/25700. Average loss: 0.2656474148730437\n",
      "5100/25700. Average loss: 0.2678726223980387\n",
      "5400/25700. Average loss: 0.2655232570009927\n",
      "5700/25700. Average loss: 0.2728103396855295\n",
      "6000/25700. Average loss: 0.25621355017026265\n",
      "6300/25700. Average loss: 0.2680250924422095\n",
      "6600/25700. Average loss: 0.2600965336461862\n",
      "6900/25700. Average loss: 0.2624358796638747\n",
      "7200/25700. Average loss: 0.2544131588221838\n",
      "7500/25700. Average loss: 0.2596119312942028\n",
      "7800/25700. Average loss: 0.26109435946991044\n",
      "8100/25700. Average loss: 0.26802797177185617\n",
      "8400/25700. Average loss: 0.2666853989039858\n",
      "8700/25700. Average loss: 0.27352483827620744\n",
      "9000/25700. Average loss: 0.26338255653778714\n",
      "9300/25700. Average loss: 0.27125744335508595\n",
      "9600/25700. Average loss: 0.25814560415844123\n",
      "9900/25700. Average loss: 0.26781509573881823\n",
      "10200/25700. Average loss: 0.27348887469309074\n",
      "10500/25700. Average loss: 0.2531442397584518\n",
      "10800/25700. Average loss: 0.2624022334565719\n",
      "11100/25700. Average loss: 0.272200454591463\n",
      "11400/25700. Average loss: 0.27458659207758807\n",
      "11700/25700. Average loss: 0.23954209197933476\n",
      "12000/25700. Average loss: 0.2593615832303961\n",
      "12300/25700. Average loss: 0.24894525011380514\n",
      "12600/25700. Average loss: 0.2608967736860116\n",
      "12900/25700. Average loss: 0.26917905011524756\n",
      "13200/25700. Average loss: 0.2723586808393399\n",
      "13500/25700. Average loss: 0.25791364289199314\n",
      "13800/25700. Average loss: 0.2788256064429879\n",
      "14100/25700. Average loss: 0.25024117089807985\n",
      "14400/25700. Average loss: 0.25687174463644624\n",
      "14700/25700. Average loss: 0.25975259751081464\n",
      "15000/25700. Average loss: 0.2829951869898165\n",
      "15300/25700. Average loss: 0.27298691468934216\n",
      "15600/25700. Average loss: 0.2627678861034413\n",
      "15900/25700. Average loss: 0.2718447043591489\n",
      "16200/25700. Average loss: 0.25257412672663726\n",
      "16500/25700. Average loss: 0.2761876826733351\n",
      "16800/25700. Average loss: 0.2647438284009695\n",
      "17100/25700. Average loss: 0.25999284800762934\n",
      "17400/25700. Average loss: 0.26810766824831567\n",
      "17700/25700. Average loss: 0.25379222561915715\n",
      "18000/25700. Average loss: 0.26208700151493153\n",
      "18300/25700. Average loss: 0.27425579078495504\n",
      "18600/25700. Average loss: 0.26747326077272493\n",
      "18900/25700. Average loss: 0.2627171847348412\n",
      "19200/25700. Average loss: 0.2677900912674765\n",
      "19500/25700. Average loss: 0.2871229186219474\n",
      "19800/25700. Average loss: 0.2688220147974789\n",
      "20100/25700. Average loss: 0.2654917448262374\n",
      "20400/25700. Average loss: 0.2613679216677944\n",
      "20700/25700. Average loss: 0.26435544531792404\n",
      "21000/25700. Average loss: 0.2664380728205045\n",
      "21300/25700. Average loss: 0.26071541287004946\n",
      "21600/25700. Average loss: 0.25326698009545606\n",
      "21900/25700. Average loss: 0.26431544138118623\n",
      "22200/25700. Average loss: 0.25808242843175927\n",
      "22500/25700. Average loss: 0.26382712005327147\n",
      "22800/25700. Average loss: 0.2639126953606804\n",
      "23100/25700. Average loss: 0.2559601900074631\n",
      "23400/25700. Average loss: 0.277308874707669\n",
      "23700/25700. Average loss: 0.27235750907411177\n",
      "24000/25700. Average loss: 0.2667874245531857\n",
      "24300/25700. Average loss: 0.2576491048062841\n",
      "24600/25700. Average loss: 0.26771597067515057\n",
      "24900/25700. Average loss: 0.271580084686478\n",
      "25200/25700. Average loss: 0.27914476906259855\n",
      "25500/25700. Average loss: 0.2564448900241405\n"
     ]
    }
   ],
   "source": [
    "print_every = 300\n",
    "total_loss = 0\n",
    "all_losses = []\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "model.train()\n",
    "\n",
    "for idx, row in train_data.iterrows():\n",
    "    text_parts = preprocess_text(str(row['text']))\n",
    "    label = torch.tensor([row['is_fake']]).long().to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    overall_output = torch.zeros((1, 2)).float().to(device)\n",
    "    for part in text_parts:\n",
    "        if len(part) > 0:\n",
    "            try:\n",
    "                input = part.reshape(-1)[:512].reshape(1, -1)\n",
    "                # print(input.shape)\n",
    "                overall_output += model(input, labels=label)[1].float().to(device)\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "\n",
    "#     overall_output /= len(text_parts)\n",
    "    overall_output = F.softmax(overall_output[0], dim=-1)\n",
    "\n",
    "    if label == 0:\n",
    "        label = torch.tensor([1.0, 0.0]).float().to(device)\n",
    "    elif label == 1:\n",
    "        label = torch.tensor([0.0, 1.0]).float().to(device)\n",
    "\n",
    "    # print(overall_output, label)\n",
    "\n",
    "    loss = criterion(overall_output, label)\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if idx % print_every == 0 and idx > 0:\n",
    "        average_loss = total_loss / print_every\n",
    "        print(\"{}/{}. Average loss: {}\".format(idx, len(train_data), average_loss))\n",
    "        all_losses.append(average_loss)\n",
    "        total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/8567. Current accuracy: 0.5133333333333333\n",
      "600/8567. Current accuracy: 0.49833333333333335\n",
      "900/8567. Current accuracy: 0.4922222222222222\n",
      "1200/8567. Current accuracy: 0.49666666666666665\n",
      "1500/8567. Current accuracy: 0.49466666666666664\n",
      "1800/8567. Current accuracy: 0.49444444444444446\n",
      "2100/8567. Current accuracy: 0.48857142857142855\n",
      "2400/8567. Current accuracy: 0.49083333333333334\n",
      "2700/8567. Current accuracy: 0.4862962962962963\n",
      "3000/8567. Current accuracy: 0.49166666666666664\n",
      "3300/8567. Current accuracy: 0.4896969696969697\n",
      "3600/8567. Current accuracy: 0.4938888888888889\n",
      "3900/8567. Current accuracy: 0.4958974358974359\n",
      "4200/8567. Current accuracy: 0.49547619047619046\n",
      "4500/8567. Current accuracy: 0.4955555555555556\n",
      "4800/8567. Current accuracy: 0.49395833333333333\n",
      "5100/8567. Current accuracy: 0.49745098039215685\n",
      "5400/8567. Current accuracy: 0.4998148148148148\n",
      "5700/8567. Current accuracy: 0.4992982456140351\n",
      "6000/8567. Current accuracy: 0.499\n",
      "6300/8567. Current accuracy: 0.5003174603174603\n",
      "6600/8567. Current accuracy: 0.4990909090909091\n",
      "6900/8567. Current accuracy: 0.49971014492753624\n",
      "7200/8567. Current accuracy: 0.49916666666666665\n",
      "7500/8567. Current accuracy: 0.49746666666666667\n",
      "7800/8567. Current accuracy: 0.498974358974359\n",
      "8100/8567. Current accuracy: 0.49864197530864196\n",
      "8400/8567. Current accuracy: 0.4990476190476191\n",
      "Accuracy on test data: 0.4990078207073655\n"
     ]
    }
   ],
   "source": [
    "total = len(test_data)\n",
    "number_right = 0\n",
    "model.eval()\n",
    "# 서버의 메모리를 아끼기 위해\n",
    "# 예측을 실행하기 전에 그라디언트를 정보를 저장하지 않는다.\n",
    "# torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    for idx, row in test_data.iterrows():\n",
    "        text_parts = preprocess_text(str(row['text']))\n",
    "        label = torch.tensor([row['is_fake']]).float().to(device)\n",
    "        \n",
    "        overall_output = torch.zeros((1,2)).to(device)\n",
    "        try:\n",
    "            for part in text_parts:\n",
    "                if len(part) > 0:\n",
    "                    overall_output += model(part.reshape(1, -1))[0]\n",
    "        except RuntimeError:\n",
    "            print(\"GPU out of memory, skipping this entry.\")\n",
    "            continue\n",
    "            \n",
    "        overall_output = F.softmax(overall_output[0], dim=-1)\n",
    "            \n",
    "        result = overall_output.max(0)[1].float().item()\n",
    " \n",
    "        if result == label.item():\n",
    "            number_right += 1\n",
    "            \n",
    "        if idx % print_every == 0 and idx > 0:\n",
    "            print(\"{}/{}. Current accuracy: {}\".format(idx, total, number_right / idx))\n",
    "            \n",
    "print(\"Accuracy on test data: {}\".format(number_right / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_softmax = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "model_softmax.config.num_labels = 1\n",
    "\n",
    "model_softmax.classifier = nn.Sequential(\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "model_softmax = model_softmax.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/25700. Average loss: 0.2631356908380985\n",
      "600/25700. Average loss: 0.27407419759159285\n",
      "900/25700. Average loss: 0.265136612476781\n",
      "1200/25700. Average loss: 0.28613990544651946\n",
      "1500/25700. Average loss: 0.26613716809699933\n",
      "1800/25700. Average loss: 0.2824543362048765\n",
      "2100/25700. Average loss: 0.2808614862306664\n",
      "2400/25700. Average loss: 0.26764260774788756\n",
      "2700/25700. Average loss: 0.2842236883038034\n",
      "3000/25700. Average loss: 0.27545256599163015\n",
      "3300/25700. Average loss: 0.25777502354234455\n",
      "3600/25700. Average loss: 0.2715592804861565\n",
      "3900/25700. Average loss: 0.26904851710423827\n",
      "4200/25700. Average loss: 0.28042380169034004\n",
      "4500/25700. Average loss: 0.2626433078447978\n",
      "4800/25700. Average loss: 0.26775248725588124\n",
      "5100/25700. Average loss: 0.27470680040307344\n",
      "5400/25700. Average loss: 0.2691542482453709\n",
      "5700/25700. Average loss: 0.27795392227359117\n",
      "6000/25700. Average loss: 0.2630182924432059\n",
      "6300/25700. Average loss: 0.27478826384991406\n",
      "6600/25700. Average loss: 0.2676382445109387\n",
      "6900/25700. Average loss: 0.2659560758185883\n",
      "7200/25700. Average loss: 0.2537675650479893\n",
      "7500/25700. Average loss: 0.2710833570733666\n",
      "7800/25700. Average loss: 0.26156124665712316\n",
      "8100/25700. Average loss: 0.2757899548051258\n",
      "8400/25700. Average loss: 0.274995152571549\n",
      "8700/25700. Average loss: 0.2825289543904364\n",
      "9000/25700. Average loss: 0.26981746272494395\n",
      "9300/25700. Average loss: 0.2736681527954837\n",
      "9600/25700. Average loss: 0.2647202996350825\n",
      "9900/25700. Average loss: 0.27417329864576456\n",
      "10200/25700. Average loss: 0.28852922095606726\n",
      "10500/25700. Average loss: 0.2590538596113523\n",
      "10800/25700. Average loss: 0.26618240543330707\n",
      "11100/25700. Average loss: 0.2846048547203342\n",
      "11400/25700. Average loss: 0.28052958187336724\n",
      "11700/25700. Average loss: 0.24652437007054687\n",
      "12000/25700. Average loss: 0.26717547003179787\n",
      "12300/25700. Average loss: 0.2525667679558198\n",
      "12600/25700. Average loss: 0.26282875599960487\n",
      "12900/25700. Average loss: 0.27658071419845026\n",
      "13200/25700. Average loss: 0.2798555624174575\n",
      "13500/25700. Average loss: 0.26731913587699335\n",
      "13800/25700. Average loss: 0.29983022513488927\n",
      "14100/25700. Average loss: 0.25331257581710814\n",
      "14400/25700. Average loss: 0.26956319199874995\n",
      "14700/25700. Average loss: 0.2646226083673537\n",
      "15000/25700. Average loss: 0.2874965782277286\n",
      "15300/25700. Average loss: 0.2725640143578251\n",
      "15600/25700. Average loss: 0.2760358121059835\n",
      "15900/25700. Average loss: 0.27623139063517255\n",
      "16200/25700. Average loss: 0.2572642829765876\n",
      "16500/25700. Average loss: 0.28687168438918886\n",
      "16800/25700. Average loss: 0.2598165149241686\n",
      "17100/25700. Average loss: 0.2729829466653367\n",
      "17400/25700. Average loss: 0.27246864076703786\n",
      "17700/25700. Average loss: 0.2678238490099708\n",
      "18000/25700. Average loss: 0.2726885215751827\n",
      "18300/25700. Average loss: 0.27561376184845965\n",
      "18600/25700. Average loss: 0.2788588090116779\n",
      "18900/25700. Average loss: 0.2695085294793049\n",
      "19200/25700. Average loss: 0.2743476337256531\n",
      "19500/25700. Average loss: 0.29179899881904325\n",
      "19800/25700. Average loss: 0.2693194126586119\n",
      "20100/25700. Average loss: 0.2771870721379916\n",
      "20400/25700. Average loss: 0.2640650452921788\n",
      "20700/25700. Average loss: 0.26722163539379834\n",
      "21000/25700. Average loss: 0.27456236992031335\n",
      "21300/25700. Average loss: 0.27339667807022733\n",
      "21600/25700. Average loss: 0.26248683760563535\n",
      "21900/25700. Average loss: 0.2689076878130436\n",
      "22200/25700. Average loss: 0.2670776556028674\n",
      "22500/25700. Average loss: 0.2667119039284686\n",
      "22800/25700. Average loss: 0.2783533245200912\n",
      "23100/25700. Average loss: 0.2565094117913395\n",
      "23400/25700. Average loss: 0.28623582399760683\n",
      "23700/25700. Average loss: 0.2801660341831545\n",
      "24000/25700. Average loss: 0.2792429559553663\n",
      "24300/25700. Average loss: 0.2656186962686479\n",
      "24600/25700. Average loss: 0.26983694834945104\n",
      "24900/25700. Average loss: 0.2848833484388888\n",
      "25200/25700. Average loss: 0.2894355849176645\n",
      "25500/25700. Average loss: 0.25996908226050436\n"
     ]
    }
   ],
   "source": [
    "print_every = 300\n",
    "total_loss = 0\n",
    "all_losses_softmax = []\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "model_softmax.train()\n",
    "\n",
    "for idx, row in train_data.iterrows():\n",
    "    text_parts = preprocess_text(str(row['text']))\n",
    "    label = torch.tensor([row['is_fake']]).long().to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    overall_output = torch.zeros((1, 2)).float().to(device)\n",
    "    for part in text_parts:\n",
    "        if len(part) > 0:\n",
    "            try:\n",
    "                input = part.reshape(-1)[:512].reshape(1, -1)\n",
    "                # print(input.shape)\n",
    "                overall_output += model(input, labels=label)[1].float().to(device)\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "\n",
    "#     overall_output /= len(text_parts)\n",
    "    overall_output = F.softmax(overall_output[0], dim=-1)\n",
    "\n",
    "    if label == 0:\n",
    "        label = torch.tensor([1.0, 0.0]).float().to(device)\n",
    "    elif label == 1:\n",
    "        label = torch.tensor([0.0, 1.0]).float().to(device)\n",
    "\n",
    "    # print(overall_output, label)\n",
    "\n",
    "    loss = criterion(overall_output, label)\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if idx % print_every == 0 and idx > 0:\n",
    "        average_loss = total_loss / print_every\n",
    "        print(\"{}/{}. Average loss: {}\".format(idx, len(train_data), average_loss))\n",
    "        all_losses.append(average_loss)\n",
    "        total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/8567. Current accuracy: 0.5133333333333333\n",
      "600/8567. Current accuracy: 0.49833333333333335\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-175401825066>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mtext_parts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'is_fake'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-13858c75f506>\u001b[0m in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_cuts\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mtext_part\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mparts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_part\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mparts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total = len(test_data)\n",
    "number_right = 0\n",
    "model_softmax.eval()\n",
    "# 서버의 메모리를 아끼기 위해\n",
    "# 예측을 실행하기 전에 그라디언트를 정보를 저장하지 않는다.\n",
    "# torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    for idx, row in test_data.iterrows():\n",
    "        text_parts = preprocess_text(str(row['text']))\n",
    "        label = torch.tensor([row['is_fake']]).float().to(device)\n",
    "        \n",
    "        overall_output = torch.zeros((1,2)).to(device)\n",
    "        try:\n",
    "            for part in text_parts:\n",
    "                if len(part) > 0:\n",
    "                    overall_output += model(part.reshape(1, -1))[0]\n",
    "        except RuntimeError:\n",
    "            print(\"GPU out of memory, skipping this entry.\")\n",
    "            continue\n",
    "            \n",
    "        overall_output = F.softmax(overall_output[0], dim=-1)\n",
    "            \n",
    "        result = overall_output.max(0)[1].float().item()\n",
    " \n",
    "        if result == label.item():\n",
    "            number_right += 1\n",
    "            \n",
    "        if idx % print_every == 0 and idx > 0:\n",
    "            print(\"{}/{}. Current accuracy: {}\".format(idx, total, number_right / idx))\n",
    "            \n",
    "print(\"Accuracy on test data: {}\".format(number_right / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
